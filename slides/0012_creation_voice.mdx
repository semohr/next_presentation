---
layout: column
section: Features
---

<img src="voice_kidnapping.png" alt="Voice Kidnapping" />

<Note>
Maybe you have seen this before, two months ago in April there were multiple attempts to blackmail
some people by using a cloned voice of there relatives. We dont know if this has happend since, but I can image that this will happen more often in the future and might be very successful.

With current models (VALL-E), you only need 3 seconds of audio to create a voice
clone of someone https://arxiv.org/pdf/2301.02111.pdf and

If we play the devils advocate for a bit, we can easily imagine a number of
scams that could be pulled off with this technology. For example, imagine
receiving a phone call from your mother, father, or child, saying that they need
some money. Or thinking about your grandparents, who are not so tech-savvy. You
could come up with a number of scenarios where they could be tricked into giving
away their money.

Additionally, it is not very difficult to set these things up. You can find
multiple open-source implementations of these models on GitHub, or there are
variety of companies that offer this as a service.

Maybe we need additional prodection for this next generation of scams,
especially for the not too tech-savvy people. Some more regulation might be
needed here to make this more difficult to pull off.

As a criminal we can now easily create a voice clone and potentially in the
future an avatar clone of someone and use it to scam people.

This leads to content based authentication to be completly broken. There is no
way at the moment to be sure that a person is real the person it pretends to be.
But this will also get more and more difficult in the future. If we cant be sure
that a person is real. There is no algorithm for truth.

</Note>

<Navbar sections={false} />
